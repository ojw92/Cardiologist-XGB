{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bab24dd4",
   "metadata": {},
   "source": [
    "# Spark_ETL.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a865cc1",
   "metadata": {},
   "source": [
    "#### This file is an experimental trial of implementing PySpark in the data preprocessing steps to create a Spark pipeline for the purpose of understanding the workflow of Spark implementation in data preprocessing. The proper order in which the files should be executed is with data_preprocessing.ipynb at the start, followed by this notebook, then test_train_datasets.ipynb."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9555445",
   "metadata": {},
   "source": [
    "### Key components:\n",
    "- schema enforcement\n",
    "- windowed de-dup\n",
    "- quantile-based winsorization\n",
    "- imputation\n",
    "- categorical encoding\n",
    "- clean persistence to Parquet\n",
    "##### Parquet is an open-source, columnar data storage format commonly used in big data ecosystems like Apache Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf84ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F, types as T, Window\n",
    "from pyspark.ml.feature import Imputer, StringIndexer, OneHotEncoder\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4537c4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a Spark session\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"cardiologist-xgb-etl\")\n",
    "         .config(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f34abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns\n",
    "# State; Sex; GeneralHealth; PhysicalHealthDays; MentalHealthDays; LastCheckupTime; PhysicalActivities; SleepHours; RemovedTeeth; HadHeartAttack; HadAngina; HadStroke; HadAsthma; HadSkinCancer; HadCOPD; HadDepressiveDisorder; HadKidneyDisease; HadArthritis; HadDiabetes DeafOrHardOfHearing; BlindOrVisionDifficulty; DifficultyConcentrating; DifficultyWalking; DifficultyDressingBathing; DifficultyErrands; SmokerStatus; ECigaretteUsage; ChestScan; RaceEthnicityCategory; AgeCategory; HeightInMeters; WeightInKilograms; BMI; AlcoholDrinkers; HIVTesting; FluVaxLast12; PneumoVaxEver; TetanusLast10Tdap; HighRiskLastYear; CovidPos;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee6a472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up an explicit schema - edit these!\n",
    "schema = T.StructType([\n",
    "    T.StructField(\"State\", T.StringType()),\n",
    "    T.StructField(\"Sex\", T.StringType()),\n",
    "    T.StructField(\"GeneralHealth\", T.StringType()),\n",
    "    T.StructField(\"PhysicalHealthDays\", T.IntegerType()),\n",
    "    T.StructField(\"MentalHealthDays\", T.IntegerType()),\n",
    "    T.StructField(\"LastCheckupTime\", T.StringType()),\n",
    "    T.StructField(\"PhysicalActivities\", T.StringType()),\n",
    "    T.StructField(\"SleepHours\", T.IntegerType()),\n",
    "    T.StructField(\"RemovedTeeth\", T.StringType()),\n",
    "    T.StructField(\"HadHeartAttack\", T.StringType()),\n",
    "    T.StructField(\"HadAngina\", T.StringType()),\n",
    "    T.StructField(\"HadStroke\", T.StringType()),\n",
    "    T.StructField(\"HadAsthma\", T.StringType()),\n",
    "    T.StructField(\"HadSkinCancer\", T.StringType()),\n",
    "    # ...\n",
    "    # add more as needed\n",
    "    # ...\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fa8855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "raw = (spark.read\n",
    "       .option(\"header\", True)\n",
    "       .schema(schema)\n",
    "       .option(\"mode\", \"PERMISSIVE\")             # keep bad rows instead of failing\n",
    "       .csv(\"../Data/heart_2022_no_nans.csv\"))   # 81 MB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa223abd",
   "metadata": {},
   "source": [
    "### Canonicalization - convert multiple variations of data into a single, standard format to ensure consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1983c121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quarantine malformed rows\n",
    "bad = raw.filter(F.col(\"SleepHours\").isNull() & F.col(\"_corrupt_record\").isNotNull()) if \"_corrupt_record\" in raw.columns else spark.createDataFrame([], schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6908767e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Canonicalize & light fixes - edit these!\n",
    "df = raw.withColumn(\"Sex\", F.trim(F.lower(F.col(\"Sex\")))) \\\n",
    "        .replace({\"m\":\"Male\",\"f\":\"Female\"}, subset=[\"Sex\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26854a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deduplicate - keep latest per patient\n",
    "#w = Window.partitionBy(\"patient_id\").orderBy(F.col(\"event_ts\").desc())\n",
    "#df_latest = (df.withColumn(\"rn\", F.row_number().over(w))                    # row_number window function\n",
    "#               .filter(\"rn = 1\").drop(\"rn\"))                                # keep latest record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74137762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier winsorization via approxQuantile\n",
    "num_cols = [\"PhysicalHealthDays\", \"MentalHealthDays\", \"SleepHours\", \"BMI\"]\n",
    "q = {}\n",
    "for c in num_cols:\n",
    "    lo, hi = df_latest.approxQuantile(c, [0.01, 0.99], 0.01)  # 1%/99% caps\n",
    "    q[c] = (lo, hi)\n",
    "for c,(lo,hi) in q.items():\n",
    "    df_latest = df_latest.withColumn(c, F.when(F.col(c) < lo, lo)\n",
    "                                             .when(F.col(c) > hi, hi)\n",
    "                                             .otherwise(F.col(c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec19193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute numeric nulls\n",
    "imputer = Imputer(inputCols=num_cols, outputCols=[f\"{c}_imp\" for c in num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a965df81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categoricals\n",
    "cat_cols = [\"Sex\"]              # add more as needed\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=f\"{c}_idx\", handleInvalid=\"keep\") for c in cat_cols]\n",
    "encoders = [OneHotEncoder(inputCols=[f\"{c}_idx\"], outputCols=[f\"{c}_oh\"], dropLast=False) for c in cat_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058a2121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline\n",
    "pipe = Pipeline(stages=[imputer] + indexers + encoders)     # include categorical encoding\n",
    "fitted = pipe.fit(df_latest)\n",
    "silver = fitted.transform(df_latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93480c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional\n",
    "# Create domain features; can create bins for numerical values\n",
    "#silver = silver.withColumn(\"age_bin\", F.when(F.col(\"age\") < 40, \"under40\")\n",
    "#                                      .when((F.col(\"age\") >= 40) & (F.col(\"age\") < 55), \"40_54\")\n",
    "#                                      .when((F.col(\"age\") >= 55) & (F.col(\"age\") < 70), \"55_69\")\n",
    "#                                      .otherwise(\"70_plus\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7dbd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist “silver” layer\n",
    "(silver\n",
    " .write.mode(\"overwrite\")\n",
    " .partitionBy()   # add a partition if you have a date\n",
    " .parquet(\"data/silver/patient_snapshot\"))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
